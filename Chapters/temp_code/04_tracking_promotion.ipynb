{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2c41d03-7753-4b40-b588-1ad893fad4f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Chapter 13 — Tracking-Based Promotion Decision\n",
    "\n",
    "This notebook demonstrates how to use MLflow Tracking data to make\n",
    "automated promotion decisions. We query the experiment for the\n",
    "best-performing run, compare it against thresholds, and promote\n",
    "only if it qualifies.\n",
    "\n",
    "**Continues from:** `03_evaluate_agent` (evaluation runs logged with metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c8d462a-683d-423a-93f5-9e1aca7d3dd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ea84e37-e9d2-4218-9c94-e154144c4fb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Set Up Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb9d1a2b-9967-4364-a396-11b056f768c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = \"demo\"\n",
    "SCHEMA  = \"finance\"\n",
    "try:\n",
    "    username = spark.conf.get(\"spark.databricks.notebook.userName\")\n",
    "except:\n",
    "    username = \"unknown\"\n",
    "if username == \"unknown\":\n",
    "    username = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "print(f\"Current user: {username}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99249967-606b-4ce8-85a2-74b8496f7096",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Query Experiment Runs and Promote the Winner\n",
    "\n",
    "This turns promotion from a manual judgment call into an automated gate\n",
    "in the CI/CD pipeline. We filter for runs that meet quality thresholds,\n",
    "rank by latency, and promote the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46c44faf-71c4-4c28-bbfe-ed449181fe2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "experiment = mlflow.get_experiment_by_name(\n",
    "    f\"/Users/{username}/data_quality_agent_experiment\"\n",
    ")\n",
    "\n",
    "# Find the run with the lowest latency that produced enough rules\n",
    "best_runs = client.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    filter_string=\"metrics.rules_generated >= 5\",  # <1>\n",
    "    order_by=[\"metrics.latency_seconds ASC\"],\n",
    "    max_results=1\n",
    ")\n",
    "\n",
    "best = best_runs[0]\n",
    "print(f\"Best run: {best.info.run_id}\")\n",
    "print(f\"  Latency: {best.data.metrics['latency_seconds']:.2f}s\")\n",
    "print(f\"  Rules:   {int(best.data.metrics['rules_generated'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26244e61-1230-4b8f-87d8-fed55ab51166",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Register and Promote the Winning Run\n",
    "\n",
    "The alias update is what causes the production serving endpoint to pick\n",
    "up the new version — no redeployment needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4a454ab-7203-4841-bca9-de29444f109d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 8"
    }
   },
   "outputs": [],
   "source": [
    "model_name = f\"{CATALOG}.{SCHEMA}.data_quality_agent\"\n",
    "\n",
    "# Since the experimental runs don't have logged models, we promote the latest\n",
    "# registered version if the best experimental run meets our criteria\n",
    "if best.data.metrics['latency_seconds'] < 10 and best.data.metrics['rules_generated'] >= 5:\n",
    "    # Get the latest version\n",
    "    versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "    latest_version = max([int(v.version) for v in versions])\n",
    "    \n",
    "    client.set_registered_model_alias(model_name, \"champion\", latest_version) # <2>\n",
    "    print(f\"Promoted version {latest_version} to 'champion' based on experimental criteria\")\n",
    "    print(f\"  Best run {best.info.run_id} met thresholds:\")\n",
    "    print(f\"    Latency: {best.data.metrics['latency_seconds']:.2f}s < 10s\")\n",
    "    print(f\"    Rules: {int(best.data.metrics['rules_generated'])} >= 5\")\n",
    "else:\n",
    "    print(\"Best run did not meet promotion criteria\")\n",
    "    print(f\"  Latency: {best.data.metrics['latency_seconds']:.2f}s\")\n",
    "    print(f\"  Rules: {int(best.data.metrics['rules_generated'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c21fbc0-71a1-43fa-a4ad-94f7213f334f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_tracking_promotion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}