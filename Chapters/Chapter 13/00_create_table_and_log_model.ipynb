{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7add11dd-b586-4799-a934-7394ef8b3cee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Chapter 13 - Step 0: Create Table and Log Model\n",
    "\n",
    "This notebook corresponds to the **Models - Standardized Packaging Format** section\n",
    "of Chapter 13. It demonstrates:\n",
    "\n",
    "1. Creating a `sales_transactions` table with synthetic data\n",
    "2. Logging the data quality agent as an MLflow PyFunc model\n",
    "3. Loading the model and generating data quality rules\n",
    "\n",
    "**Prerequisites:**\n",
    "- Upload the `data_quality_agent_model/` folder to the same workspace directory as this notebook\n",
    "- The `databricks-meta-llama-3-3-70b-instruct` foundation model endpoint must be available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11bfa804-2574-4165-875b-45999bee913a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6b7bda8-993d-49b6-a287-624fc8fda284",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuration\n",
    "\n",
    "Set the catalog and schema where the demo table and model will live.\n",
    "Change these to match your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c880f7f4-de41-4721-8614-c77d1c78fa77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = \"demo\"\n",
    "SCHEMA  = \"finance\"\n",
    "TABLE   = \"sales_transactions\"\n",
    "\n",
    "try:\n",
    "    username = spark.conf.get(\"spark.databricks.notebook.userName\")\n",
    "except:\n",
    "    username = \"unknown\"\n",
    "if username == \"unknown\":\n",
    "    username = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "print(f\"Current user: {username}\")\n",
    "\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {SCHEMA}\")\n",
    "\n",
    "print(f\"Using {CATALOG}.{SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9133f2a5-c0cc-4784-85de-066c71c7ae8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1 - Create the sales_transactions table with synthetic data\n",
    "\n",
    "The table is designed to give the agent interesting columns to reason about:\n",
    "numeric ranges, date ordering, email formats, enum values, and cross-column\n",
    "relationships. A small percentage of rows intentionally contain data quality\n",
    "issues so the agent has real violations to detect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4944f11c-dfd6-4862-805e-47ad5f6e317f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType,\n",
    "    DoubleType, DateType\n",
    ")\n",
    "import random\n",
    "from datetime import date, timedelta\n",
    "\n",
    "NUM_ROWS = 2000\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "start_date = date(2024, 1, 1)\n",
    "statuses = [\"completed\", \"pending\", \"cancelled\", \"refunded\"]\n",
    "payment_methods = [\"credit_card\", \"debit_card\", \"wire_transfer\", \"paypal\"]\n",
    "domains = [\"example.com\", \"testmail.org\", \"acme.co\", \"bigcorp.net\"]\n",
    "\n",
    "def random_email(i):\n",
    "    return f\"customer_{i}@{random.choice(domains)}\"\n",
    "\n",
    "def random_date_pair():\n",
    "    \"\"\"Return an (order_date, ship_date) pair where ship >= order most of the time.\"\"\"\n",
    "    order = start_date + timedelta(days=random.randint(0, 365))\n",
    "    # ~5% of rows intentionally have ship_date before order_date\n",
    "    if random.random() < 0.05:\n",
    "        ship = order - timedelta(days=random.randint(1, 5))\n",
    "    else:\n",
    "        ship = order + timedelta(days=random.randint(0, 14))\n",
    "    return order, ship\n",
    "\n",
    "rows = []\n",
    "for i in range(1, NUM_ROWS + 1):\n",
    "    original_price = round(random.uniform(5.0, 500.0), 2)\n",
    "    # ~3% of rows have discount > original (data quality issue)\n",
    "    if random.random() < 0.03:\n",
    "        discount_price = round(original_price + random.uniform(1, 50), 2)\n",
    "    else:\n",
    "        discount_price = round(original_price * random.uniform(0.5, 1.0), 2)\n",
    "\n",
    "    quantity = random.randint(1, 200)\n",
    "    # ~2% negative quantities (data quality issue)\n",
    "    if random.random() < 0.02:\n",
    "        quantity = -random.randint(1, 10)\n",
    "\n",
    "    order_dt, ship_dt = random_date_pair()\n",
    "\n",
    "    rows.append((\n",
    "        f\"TXN-{i:06d}\",\n",
    "        random_email(i),\n",
    "        round(original_price * quantity, 2),\n",
    "        original_price,\n",
    "        discount_price,\n",
    "        quantity,\n",
    "        order_dt,\n",
    "        ship_dt,\n",
    "        random.choice(statuses),\n",
    "        random.choice(payment_methods),\n",
    "    ))\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"transaction_id\",  StringType(),  False),\n",
    "    StructField(\"customer_email\",  StringType(),  True),\n",
    "    StructField(\"amount\",          DoubleType(),  True),\n",
    "    StructField(\"original_price\",  DoubleType(),  True),\n",
    "    StructField(\"discount_price\",  DoubleType(),  True),\n",
    "    StructField(\"quantity\",        IntegerType(), True),\n",
    "    StructField(\"order_date\",      DateType(),    True),\n",
    "    StructField(\"ship_date\",       DateType(),    True),\n",
    "    StructField(\"status\",          StringType(),  True),\n",
    "    StructField(\"payment_method\",  StringType(),  True),\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(rows, schema)\n",
    "df.write.mode(\"overwrite\").saveAsTable(TABLE)\n",
    "\n",
    "print(f\"Created {CATALOG}.{SCHEMA}.{TABLE} with {df.count()} rows\")\n",
    "display(df.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6e82e85-fc2a-4413-98d2-da57f62bdcfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2 - Log the data quality agent as an MLflow model\n",
    "\n",
    "We use `mlflow.pyfunc.log_model` to package the agent code, artifacts, and\n",
    "dependencies into a single versioned unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4d84026-6af6-4ebb-87f4-1f55410c4dea",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 8"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# Add the code path to sys.path so we can import the model\n",
    "sys.path.insert(0, \"data_quality_agent_model/code\")\n",
    "\n",
    "# Import and instantiate the model class\n",
    "from data_quality_agent_model.python_model import DataQualityAgentModel\n",
    "\n",
    "# Signature: what the model expects and returns\n",
    "sample_input = pd.DataFrame({\n",
    "    \"table_name\": [TABLE],\n",
    "    \"catalog\": [CATALOG],\n",
    "    \"schema\": [SCHEMA]\n",
    "})\n",
    "sample_output = pd.DataFrame({\n",
    "    \"rules\": ['{\"column\": \"amount\", \"rule\": \"amount > 0\"}']\n",
    "})\n",
    "signature = infer_signature(sample_input, sample_output)\n",
    "\n",
    "# Set experiment\n",
    "mlflow.set_experiment(f\"/Users/{username}/data_quality_agent_experiment\")\n",
    "\n",
    "# Log model with instance instead of file path\n",
    "with mlflow.start_run(run_name=\"data_quality_agent_v1\") as run:\n",
    "    mlflow.pyfunc.log_model(\n",
    "        name=\"data_quality_agent\",\n",
    "        python_model=DataQualityAgentModel(),\n",
    "        code_paths=[\"data_quality_agent_model/code/data_quality_agent\"],\n",
    "        artifacts={\n",
    "            \"system_prompt\": \"data_quality_agent_model/artifacts/system_prompt.txt\",\n",
    "            \"example_rules\": \"data_quality_agent_model/artifacts/example_rules.json\",\n",
    "        },\n",
    "        signature=signature,\n",
    "        input_example=sample_input,\n",
    "        pip_requirements=\"data_quality_agent_model/requirements.txt\",\n",
    "    )\n",
    "    run_id = run.info.run_id\n",
    "    print(f\"Model logged - run_id: {run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d35952d5-8e57-4f88-b4e8-5557f3f72787",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3 - Load the model and generate rules\n",
    "\n",
    "We load the model we just logged and ask it to generate data quality\n",
    "rules for our sales_transactions table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "899662a6-c2fb-43b2-8f0b-6aa380835a7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "model = mlflow.pyfunc.load_model(f\"runs:/{run_id}/data_quality_agent\")\n",
    "\n",
    "result = model.predict(pd.DataFrame({\n",
    "    \"table_name\": [TABLE],\n",
    "    \"catalog\": [CATALOG],\n",
    "    \"schema\": [SCHEMA],\n",
    "}))\n",
    "\n",
    "rules = json.loads(result[\"rules\"].iloc[0])\n",
    "\n",
    "print(f\"Generated {len(rules)} data quality rules:\\n\")\n",
    "for r in rules:\n",
    "    source = r.get(\"source\", \"?\")\n",
    "    print(f\"  [{source:6s}]  {r['column']:20s}  ->  {r['rule']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6909b8e-f38f-4b00-9abc-4624d1034285",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## (Optional) Sample SQL checks\n",
    "\n",
    "The generated rules can be applied as SQL checks against the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13c7bdda-ff1c-41a3-8823-07103b71f7f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Sample SQL checks:\\n\")\n",
    "for r in rules[:5]:\n",
    "    rule_expr = r[\"rule\"]\n",
    "    print(f\"SELECT COUNT(*) AS violations FROM {CATALOG}.{SCHEMA}.{TABLE} WHERE NOT ({rule_expr});\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "002385cc-3b49-4fe1-a282-b957fe1f5ba6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "The `run_id` variable contains the ID of the run we just created.\n",
    "Use **01_model_packaging** next to validate and register this model in Unity Catalog."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_create_table_and_log_model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}