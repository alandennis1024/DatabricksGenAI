{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58bbadf6-238b-4faa-a8cd-cb1863589a14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Chapter 13 â€” Evaluate the Agent and Log Metrics\n",
    "\n",
    "This notebook runs the registered agent against test data, measures\n",
    "performance, and logs metrics to the experiment. These metrics are\n",
    "what the promotion decision in `04_tracking_promotion` queries.\n",
    "\n",
    "Run this notebook multiple times with different parameters (temperature,\n",
    "prompt version, LLM endpoint) to create candidate runs for comparison.\n",
    "\n",
    "**Continues from:** `01_model_packaging` (model registered in Unity Catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4807c0f4-4d05-4175-8a7f-42901f4e483e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79e3f749-6f3d-43de-8fc1-4c3a082db843",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f7dddb3-c524-4464-b200-baea932bb800",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = \"demo\"\n",
    "SCHEMA  = \"finance\"\n",
    "TABLE   = \"sales_transactions\"\n",
    "\n",
    "try:\n",
    "    username = spark.conf.get(\"spark.databricks.notebook.userName\")\n",
    "except:\n",
    "    username = \"unknown\"\n",
    "if username == \"unknown\":\n",
    "    username = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "print(f\"Current user: {username}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4bbbdfb-eb35-4650-9c24-77b86d82d32b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Load the Registered Model\n",
    "\n",
    "We load the model from Unity Catalog rather than from a run,\n",
    "since it has already been validated and registered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ab8a79d-08e4-426e-ad6f-deaabcfe4767",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "mlflow.set_experiment(f\"/Users/{username}/data_quality_agent_experiment\")\n",
    "\n",
    "model_name = f\"{CATALOG}.{SCHEMA}.data_quality_agent\"\n",
    "model = mlflow.pyfunc.load_model(f\"models:/{model_name}@champion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06005c09-fdf8-4a3b-aa6e-9e4df16b62a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Run the Agent and Log Metrics\n",
    "\n",
    "We time the prediction, count the rules generated, and log everything\n",
    "as an MLflow run. The parameters record the configuration that produced\n",
    "these results; the metrics record how the agent performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e83550c-5ccd-4509-bbbd-4dfe96f1ae23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"agent_eval_v1\") as run:\n",
    "    # Log the parameters that shape agent behavior\n",
    "    mlflow.log_param(\"llm_endpoint\", \"databricks-meta-llama-3-3-70b-instruct\")\n",
    "    mlflow.log_param(\"temperature\", 0.1)\n",
    "    mlflow.log_param(\"max_tokens\", 1024)\n",
    "    mlflow.log_param(\"model_name\", model_name)\n",
    "\n",
    "    # Run the agent and measure performance\n",
    "    test_input = pd.DataFrame({\n",
    "        \"table_name\": [TABLE],\n",
    "        \"catalog\": [CATALOG],\n",
    "        \"schema\": [SCHEMA]\n",
    "    })\n",
    "\n",
    "    start = time.time()\n",
    "    result = model.predict(test_input)\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    import json\n",
    "    rules = json.loads(result[\"rules\"].iloc[0])\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"rules_generated\", len(rules))\n",
    "    mlflow.log_metric(\"latency_seconds\", elapsed)\n",
    "\n",
    "    print(f\"Generated {len(rules)} rules in {elapsed:.2f}s\")\n",
    "    print(f\"Run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e10685d9-d816-48e2-98f5-01a435bc2285",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. (Optional) Run Again with Different Parameters\n",
    "\n",
    "To create multiple candidate runs for the promotion decision in\n",
    "`04_tracking_promotion`, re-run the cell above after changing parameters\n",
    "(e.g., use a different model version, temperature, or prompt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de2ea4f5-3b83-4b4d-b26c-6a7a223946c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "Multiple evaluation runs now exist in the experiment with `rules_generated`\n",
    "and `latency_seconds` metrics. Use **04_tracking_promotion** to query these\n",
    "runs and promote the best one."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_evaluate_agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}