# Data Quality Agent for CobaltWorks

## Purpose
This project provides an MLflow model agent that analyzes incoming Spark DataFrames, specifically a CobaltWorks delivery manifest, and proposes data quality rules using a Large Language Model (LLM). The agent now leverages schema, column names, sample data, and comprehensive summary statistics to generate tailored rules in YAML format, aiming to ensure high data quality in the bronze-to-silver ETL pipeline.

## How It Works
- The agent receives a Spark DataFrame as input (e.g., a delivery manifest).
- It extracts the schema, column names, a sample of the data, and computes summary statistics.
- It builds a rich prompt incorporating all these data characteristics and sends it to an LLM (currently a dummy client).
- The LLM returns data quality rules in YAML format, customized for the CobaltWorks context.
- The agent outputs these rules for use in data validation or documentation.

## Usage
1. Load the MLflow model agent (see `data_quality_agent.py`).
2. Run the `main.py` script, which internally generates a sample CobaltWorks delivery manifest DataFrame.
3. Receive YAML-formatted data quality rules tailored to the manifest data.

## Customization
- Replace `DummyLLMClient` in `data_quality_agent.py` with your preferred LLM API client (e.g., OpenAI, Azure OpenAI, local LLM).
- Adjust the prompt template as needed for different CobaltWorks use cases or data sources.

## Example Output
```yaml
rules:
  - column: product_id
    rules:
      - 'is not null'
      - 'starts with CW-'
  - column: quantity
    rules:
      - 'is not null'
      - 'must be > 0'
  - column: estimated_arrival_date
    rules:
      - 'is not null'
      - 'is a valid ISO 8601 timestamp'
      - 'is not in the past'
```

## Files
- `src/data_quality_agent/data_quality_agent.py`: Main agent and MLflow wrapper implementation.
- `src/data_quality_agent/cobaltworks_data.py`: Generates sample CobaltWorks data.
- `README.md`: This documentation.

## Next Steps
- Integrate with your LLM provider.
- Extend rule generation logic for more advanced data quality checks.
- Add comprehensive unit and integration tests.

## Future Enhancements
- **Databricks Vector Search Integration:** Integrate a Vector Search index of historical rules to provide the LLM with context-specific and proven data quality patterns, improving the relevance and accuracy of proposed rules.
- **DB App for Rule Approval:** Develop a Databricks App to allow data engineers to review and approve the LLM-proposed rules, seamlessly integrating the agent into existing data governance workflows.
- **Dynamic Prompt Engineering:** Implement a more sophisticated prompt engineering strategy, potentially using MLflow Prompt Engineering, to dynamically adjust prompts based on data characteristics or user feedback.

---

# data_quality_agent

The 'data_quality_agent' project was generated by using the default-python template.

* `src/`: Python source code for this project.
  * `src/data_quality_agent/`: Shared Python code that can be used by jobs and pipelines.
* `resources/`:  Resource configurations (jobs, pipelines, etc.)
* `tests/`: Unit tests for the shared Python code.
* `fixtures/`: Fixtures for data sets (primarily used for testing).


## Getting started

Choose how you want to work on this project:

(a) Directly in your Databricks workspace, see
    https://docs.databricks.com/dev-tools/bundles/workspace.

(b) Locally with an IDE like Cursor or VS Code, see
    https://docs.databricks.com/dev-tools/vscode-ext.html.

(c) With command line tools, see https://docs.databricks.com/dev-tools/cli/databricks-cli.html

If you're developing with an IDE, dependencies for this project should be installed using uv:

*  Make sure you have the UV package manager installed.
   It's an alternative to tools like pip: https://docs.astral.sh/uv/getting-started/installation/.
*  Run `uv sync --dev` to install the project's dependencies.


# Using this project using the CLI

The Databricks workspace and IDE extensions provide a graphical interface for working
with this project. It's also possible to interact with it directly using the CLI:

1. Authenticate to your Databricks workspace, if you have not done so already:
    ```
    $ databricks configure
    ```

2. To deploy a development copy of this project, type:
    ```
    $ databricks bundle deploy --target dev
    ```
    (Note that "dev" is the default target, so the `--target` parameter
    is optional here.)

    This deploys everything that's defined for this project.
    For example, the default template would deploy a job called
    `[dev yourname] data_quality_agent_job` to your workspace.
    You can find that resource by opening your workpace and clicking on **Jobs & Pipelines**.

3. Similarly, to deploy a production copy, type:
   ```
   $ databricks bundle deploy --target prod
   ```
   Note that the default job from the template has a schedule that runs every day
   (defined in resources/sample_job.job.yml). The schedule
   is paused when deploying in development mode (see
   https://docs.databricks.com/dev-tools/bundles/deployment-modes.html).

4. To run a job or pipeline, use the "run" command:
   ```
   $ databricks bundle run
   ```

5. Finally, to run tests locally, use `pytest`:
   ```
   $ uv run pytest
   ```
